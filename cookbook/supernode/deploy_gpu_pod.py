#!/usr/bin/env python3
"""
åœ¨è¶…çº§èŠ‚ç‚¹ä¸Šéƒ¨ç½² GPU Pod ç¤ºä¾‹è„šæœ¬

åŠŸèƒ½: åœ¨ TKE è¶…çº§èŠ‚ç‚¹ä¸Šåˆ›å»º GPU Podï¼Œæ”¯æŒè‡ªåŠ¨åŒ¹é…å’Œæ‰‹åŠ¨æŒ‡å®š GPU ç±»å‹
ä½¿ç”¨æ–¹æ³•: python3 deploy_gpu_pod.py --help
æ–‡æ¡£é“¾æ¥: https://tke-workshop.github.io/ai-ml/04-gpu-pod-best-practices/
"""

import argparse
import sys
import time
from pathlib import Path
from typing import Optional, Dict, Any

sys.path.insert(0, str(Path(__file__).parent.parent))

from common.logger import setup_logger, LogContext
from kubernetes import client, config
from kubernetes.client.rest import ApiException

logger = setup_logger(__name__)


class GPUPodDeployer:
    """GPU Pod éƒ¨ç½²å™¨"""
    
    # æ”¯æŒçš„ GPU å‹å·åŠå…¶æ˜¾å­˜é…ç½®
    GPU_SPECS = {
        'V100': {'memory': '16GB', 'cuda': '11.4', 'use_case': 'é«˜æ€§èƒ½è®­ç»ƒã€å¤§æ¨¡å‹æ¨ç†'},
        'T4': {'memory': '16GB', 'cuda': '11.4', 'use_case': 'é€šç”¨æ¨ç†ã€å°æ¨¡å‹è®­ç»ƒ'},
        '1/4*T4': {'memory': '4GB', 'cuda': '11.0', 'use_case': 'è½»é‡æ¨ç†ã€å¼€å‘æµ‹è¯•'},
        '1/2*T4': {'memory': '8GB', 'cuda': '11.0', 'use_case': 'ä¸­ç­‰æ¨ç†ã€æ‰¹å¤„ç†'},
        'A10*GNV4': {'memory': '24GB', 'cuda': '11.4', 'use_case': 'AI æ¨ç†ã€å›¾å½¢æ¸²æŸ“'},
        'A10*GNV4v': {'memory': '24GB', 'cuda': '11.4', 'use_case': 'è™šæ‹ŸåŒ– GPU å·¥ä½œè´Ÿè½½'},
        'A10*PNV4': {'memory': '24GB', 'cuda': '11.4', 'use_case': 'é«˜æ€§èƒ½å›¾å½¢å’Œè®¡ç®—'},
        'L20': {'memory': '48GB', 'cuda': '12.7', 'use_case': 'é«˜ç«¯å›¾å½¢å·¥ä½œè´Ÿè½½'},
        'L40': {'memory': '48GB', 'cuda': '12.7', 'use_case': 'é«˜ç«¯å›¾å½¢å·¥ä½œè´Ÿè½½'},
    }
    
    def __init__(self, kubeconfig_path: Optional[str] = None):
        """
        åˆå§‹åŒ–éƒ¨ç½²å™¨
        
        Args:
            kubeconfig_path: kubeconfig æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        self.kubeconfig_path = kubeconfig_path
        self._load_kube_config()
        self.core_v1 = client.CoreV1Api()
        self.apps_v1 = client.AppsV1Api()
    
    def _load_kube_config(self):
        """åŠ è½½ kubeconfig"""
        try:
            if self.kubeconfig_path:
                config.load_kube_config(config_file=self.kubeconfig_path)
            else:
                config.load_kube_config()
            logger.info("âœ… å·²åŠ è½½ kubeconfig")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ kubeconfig å¤±è´¥: {e}")
            raise
    
    def create_gpu_pod(
        self,
        name: str,
        namespace: str,
        image: str,
        gpu_type: str,
        gpu_count: int = 1,
        cpu: Optional[str] = None,
        memory: Optional[str] = None,
        use_auto_match: bool = True,
        use_image_cache: bool = False,
        image_cache_id: Optional[str] = None,
        disk_size: int = 200,
        command: Optional[list] = None,
        env: Optional[Dict[str, str]] = None,
        labels: Optional[Dict[str, str]] = None
    ) -> Dict[str, Any]:
        """
        åˆ›å»º GPU Pod
        
        Args:
            name: Pod åç§°
            namespace: å‘½åç©ºé—´
            image: å®¹å™¨é•œåƒ
            gpu_type: GPU å‹å· (å¦‚ 'T4', 'V100', '1/4*T4' ç­‰)
            gpu_count: GPU æ•°é‡
            cpu: CPU æ ¸æ•°ï¼ˆæ˜¾å¼æŒ‡å®šæ—¶ä½¿ç”¨ï¼‰
            memory: å†…å­˜å¤§å°ï¼ˆæ˜¾å¼æŒ‡å®šæ—¶ä½¿ç”¨ï¼‰
            use_auto_match: æ˜¯å¦ä½¿ç”¨è‡ªåŠ¨åŒ¹é…æ¨¡å¼ï¼ˆæ¨èï¼‰
            use_image_cache: æ˜¯å¦å¯ç”¨é•œåƒç¼“å­˜
            image_cache_id: é•œåƒç¼“å­˜ IDï¼ˆæ‰‹åŠ¨æŒ‡å®šæ—¶ä½¿ç”¨ï¼‰
            disk_size: ç£ç›˜å¤§å°ï¼ˆGBï¼Œä½¿ç”¨é•œåƒç¼“å­˜æ—¶éœ€è¦ï¼‰
            command: å®¹å™¨å¯åŠ¨å‘½ä»¤
            env: ç¯å¢ƒå˜é‡
            labels: Pod æ ‡ç­¾
        
        Returns:
            åˆ›å»ºçš„ Pod ä¿¡æ¯
        """
        with LogContext(logger, f"åˆ›å»º GPU Pod: {name}"):
            # éªŒè¯ GPU å‹å·
            if gpu_type not in self.GPU_SPECS:
                raise ValueError(
                    f"ä¸æ”¯æŒçš„ GPU å‹å·: {gpu_type}\n"
                    f"æ”¯æŒçš„å‹å·: {', '.join(self.GPU_SPECS.keys())}"
                )
            
            gpu_spec = self.GPU_SPECS[gpu_type]
            logger.info(f"ğŸ“Š GPU é…ç½®: {gpu_type} ({gpu_spec['memory']}, CUDA {gpu_spec['cuda']})")
            logger.info(f"ğŸ¯ é€‚ç”¨åœºæ™¯: {gpu_spec['use_case']}")
            
            # æ„å»º annotations
            annotations = self._build_annotations(
                gpu_type=gpu_type,
                gpu_count=gpu_count,
                cpu=cpu,
                memory=memory,
                use_auto_match=use_auto_match,
                use_image_cache=use_image_cache,
                image_cache_id=image_cache_id,
                disk_size=disk_size
            )
            
            # æ„å»ºå®¹å™¨é…ç½®
            container = self._build_container(
                name=name,
                image=image,
                gpu_count=gpu_count,
                cpu=cpu,
                memory=memory,
                use_auto_match=use_auto_match,
                command=command,
                env=env
            )
            
            # æ„å»º Pod å®šä¹‰
            pod_labels = labels or {}
            pod_labels.update({
                'app': name,
                'gpu-type': gpu_type.replace('*', '-').replace('/', '-')
            })
            
            pod = client.V1Pod(
                api_version="v1",
                kind="Pod",
                metadata=client.V1ObjectMeta(
                    name=name,
                    namespace=namespace,
                    labels=pod_labels,
                    annotations=annotations
                ),
                spec=client.V1PodSpec(
                    containers=[container],
                    restart_policy="Never",
                    # è°ƒåº¦åˆ°è¶…çº§èŠ‚ç‚¹
                    node_selector={"type": "virtual-kubelet"},
                    tolerations=[
                        client.V1Toleration(
                            key="serverless",
                            operator="Exists",
                            effect="NoSchedule"
                        )
                    ]
                )
            )
            
            # åˆ›å»º Pod
            try:
                logger.info(f"ğŸš€ æ­£åœ¨åˆ›å»º Pod: {namespace}/{name}")
                api_response = self.core_v1.create_namespaced_pod(
                    namespace=namespace,
                    body=pod
                )
                
                logger.info(f"âœ… Pod åˆ›å»ºæˆåŠŸ: {api_response.metadata.name}")
                logger.info(f"ğŸ“ Pod UID: {api_response.metadata.uid}")
                
                # ç­‰å¾… Pod å¯åŠ¨
                self._wait_for_pod_ready(name, namespace)
                
                return {
                    'name': api_response.metadata.name,
                    'namespace': api_response.metadata.namespace,
                    'uid': api_response.metadata.uid,
                    'status': 'Created',
                    'gpu_type': gpu_type,
                    'gpu_count': gpu_count,
                    'image': image
                }
                
            except ApiException as e:
                logger.error(f"âŒ åˆ›å»º Pod å¤±è´¥: {e.reason}")
                if e.body:
                    logger.error(f"é”™è¯¯è¯¦æƒ…: {e.body}")
                raise
    
    def _build_annotations(
        self,
        gpu_type: str,
        gpu_count: int,
        cpu: Optional[str],
        memory: Optional[str],
        use_auto_match: bool,
        use_image_cache: bool,
        image_cache_id: Optional[str],
        disk_size: int
    ) -> Dict[str, str]:
        """æ„å»º Pod annotations"""
        annotations = {}
        
        # GPU é…ç½®
        if use_auto_match:
            # æ–¹å¼ B: è‡ªåŠ¨åŒ¹é…ï¼ˆæ¨èï¼‰
            annotations['eks.tke.cloud.tencent.com/gpu-type'] = gpu_type
            logger.info(f"ğŸ”§ ä½¿ç”¨è‡ªåŠ¨åŒ¹é…æ¨¡å¼: GPU={gpu_type}")
        else:
            # æ–¹å¼ A: æ˜¾å¼æŒ‡å®š
            if not cpu or not memory:
                raise ValueError("ä½¿ç”¨æ˜¾å¼æŒ‡å®šæ¨¡å¼æ—¶ï¼Œå¿…é¡»æä¾› cpu å’Œ memory å‚æ•°")
            
            annotations['eks.tke.cloud.tencent.com/gpu-type'] = gpu_type
            annotations['eks.tke.cloud.tencent.com/gpu-count'] = str(gpu_count)
            annotations['eks.tke.cloud.tencent.com/cpu'] = cpu
            annotations['eks.tke.cloud.tencent.com/mem'] = memory
            logger.info(f"ğŸ”§ ä½¿ç”¨æ˜¾å¼æŒ‡å®šæ¨¡å¼: GPU={gpu_type}, Count={gpu_count}, CPU={cpu}, Memory={memory}")
        
        # é•œåƒç¼“å­˜é…ç½®
        if use_image_cache:
            if image_cache_id:
                # æ‰‹åŠ¨æŒ‡å®šé•œåƒç¼“å­˜
                annotations['eks.tke.cloud.tencent.com/use-image-cache'] = image_cache_id
                logger.info(f"ğŸ’¾ ä½¿ç”¨æŒ‡å®šé•œåƒç¼“å­˜: {image_cache_id}")
            else:
                # è‡ªåŠ¨åŒ¹é…é•œåƒç¼“å­˜
                annotations['eks.tke.cloud.tencent.com/use-image-cache'] = 'auto'
                annotations['eks.tke.cloud.tencent.com/pod-resource'] = f'{{"disk": {{"size": {disk_size}}}}}'
                logger.info(f"ğŸ’¾ å¯ç”¨è‡ªåŠ¨é•œåƒç¼“å­˜: ç£ç›˜å¤§å°={disk_size}GB")
        
        return annotations
    
    def _build_container(
        self,
        name: str,
        image: str,
        gpu_count: int,
        cpu: Optional[str],
        memory: Optional[str],
        use_auto_match: bool,
        command: Optional[list],
        env: Optional[Dict[str, str]]
    ) -> client.V1Container:
        """æ„å»ºå®¹å™¨é…ç½®"""
        # ç¯å¢ƒå˜é‡
        env_vars = [
            client.V1EnvVar(name="NVIDIA_VISIBLE_DEVICES", value="all"),
            client.V1EnvVar(name="NVIDIA_DRIVER_CAPABILITIES", value="compute,utility")
        ]
        
        if env:
            env_vars.extend([
                client.V1EnvVar(name=k, value=v) for k, v in env.items()
            ])
        
        # èµ„æºé…ç½®
        resources = None
        if use_auto_match:
            # è‡ªåŠ¨åŒ¹é…æ¨¡å¼ï¼šä½¿ç”¨ resources å®šä¹‰
            resources = client.V1ResourceRequirements(
                requests={
                    "cpu": cpu or "4",
                    "memory": memory or "16Gi",
                    "nvidia.com/gpu": str(gpu_count)
                },
                limits={
                    "cpu": cpu or "8",
                    "memory": memory or "32Gi",
                    "nvidia.com/gpu": str(gpu_count)
                }
            )
        
        container = client.V1Container(
            name=name,
            image=image,
            env=env_vars
        )
        
        if command:
            container.command = command
        
        if resources:
            container.resources = resources
        
        return container
    
    def _wait_for_pod_ready(
        self,
        name: str,
        namespace: str,
        timeout: int = 600
    ):
        """ç­‰å¾… Pod å°±ç»ª"""
        logger.info(f"â³ ç­‰å¾… Pod å°±ç»ª (è¶…æ—¶: {timeout}s)...")
        
        start_time = time.time()
        last_phase = None
        
        while time.time() - start_time < timeout:
            try:
                pod = self.core_v1.read_namespaced_pod(name, namespace)
                phase = pod.status.phase
                
                if phase != last_phase:
                    logger.info(f"ğŸ“Š Pod çŠ¶æ€: {phase}")
                    last_phase = phase
                
                if phase == "Running":
                    logger.info("âœ… Pod å·²å°±ç»ª")
                    return
                elif phase == "Failed":
                    logger.error("âŒ Pod å¯åŠ¨å¤±è´¥")
                    self._log_pod_events(name, namespace)
                    raise RuntimeError(f"Pod {name} å¯åŠ¨å¤±è´¥")
                
                time.sleep(5)
                
            except ApiException as e:
                logger.error(f"âŒ æŸ¥è¯¢ Pod çŠ¶æ€å¤±è´¥: {e.reason}")
                raise
        
        logger.warning(f"âš ï¸ Pod åœ¨ {timeout}s å†…æœªå°±ç»ª")
        self._log_pod_events(name, namespace)
    
    def _log_pod_events(self, name: str, namespace: str):
        """æ‰“å° Pod äº‹ä»¶"""
        try:
            events = self.core_v1.list_namespaced_event(
                namespace=namespace,
                field_selector=f"involvedObject.name={name}"
            )
            
            if events.items:
                logger.info("ğŸ“‹ Pod äº‹ä»¶:")
                for event in events.items[-10:]:  # æ˜¾ç¤ºæœ€è¿‘ 10 æ¡
                    logger.info(
                        f"  [{event.type}] {event.reason}: {event.message}"
                    )
        except Exception as e:
            logger.warning(f"âš ï¸ è·å– Pod äº‹ä»¶å¤±è´¥: {e}")
    
    def delete_pod(self, name: str, namespace: str):
        """åˆ é™¤ Pod"""
        with LogContext(logger, f"åˆ é™¤ Pod: {namespace}/{name}"):
            try:
                self.core_v1.delete_namespaced_pod(
                    name=name,
                    namespace=namespace,
                    body=client.V1DeleteOptions()
                )
                logger.info("âœ… Pod åˆ é™¤æˆåŠŸ")
            except ApiException as e:
                if e.status == 404:
                    logger.warning("âš ï¸ Pod ä¸å­˜åœ¨")
                else:
                    logger.error(f"âŒ åˆ é™¤ Pod å¤±è´¥: {e.reason}")
                    raise
    
    def get_pod_logs(
        self,
        name: str,
        namespace: str,
        tail_lines: int = 100
    ) -> str:
        """è·å– Pod æ—¥å¿—"""
        try:
            logs = self.core_v1.read_namespaced_pod_log(
                name=name,
                namespace=namespace,
                tail_lines=tail_lines
            )
            return logs
        except ApiException as e:
            logger.error(f"âŒ è·å– Pod æ—¥å¿—å¤±è´¥: {e.reason}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='åœ¨è¶…çº§èŠ‚ç‚¹ä¸Šéƒ¨ç½² GPU Pod',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹:
  # 1. ä½¿ç”¨è‡ªåŠ¨åŒ¹é…åˆ›å»º T4 GPU Podï¼ˆæ¨èï¼‰
  python3 deploy_gpu_pod.py \\
    --name gpu-inference \\
    --image pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime \\
    --gpu-type T4 \\
    --gpu-count 1

  # 2. ä½¿ç”¨æ˜¾å¼æŒ‡å®šåˆ›å»º V100 GPU Pod
  python3 deploy_gpu_pod.py \\
    --name gpu-training \\
    --image tensorflow/tensorflow:2.13.0-gpu \\
    --gpu-type V100 \\
    --gpu-count 1 \\
    --cpu 8 \\
    --memory 40Gi \\
    --no-auto-match

  # 3. ä½¿ç”¨é•œåƒç¼“å­˜åŠ é€Ÿå¯åŠ¨
  python3 deploy_gpu_pod.py \\
    --name gpu-fast \\
    --image pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime \\
    --gpu-type T4 \\
    --use-image-cache \\
    --disk-size 200

  # 4. æ‰‹åŠ¨æŒ‡å®šé•œåƒç¼“å­˜
  python3 deploy_gpu_pod.py \\
    --name gpu-cached \\
    --image pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime \\
    --gpu-type T4 \\
    --image-cache-id imc-xxxxxxxx

  # 5. åˆ é™¤ Pod
  python3 deploy_gpu_pod.py --delete --name gpu-inference

  # 6. æŸ¥çœ‹ Pod æ—¥å¿—
  python3 deploy_gpu_pod.py --logs --name gpu-inference --tail 50
        '''
    )
    
    # æ“ä½œç±»å‹
    parser.add_argument('--delete', action='store_true', help='åˆ é™¤ Pod')
    parser.add_argument('--logs', action='store_true', help='æŸ¥çœ‹ Pod æ—¥å¿—')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--name', required=True, help='Pod åç§°')
    parser.add_argument('--namespace', default='default', help='å‘½åç©ºé—´ (é»˜è®¤: default)')
    parser.add_argument('--image', help='å®¹å™¨é•œåƒ')
    parser.add_argument('--kubeconfig', help='kubeconfig æ–‡ä»¶è·¯å¾„')
    
    # GPU é…ç½®
    parser.add_argument(
        '--gpu-type',
        choices=list(GPUPodDeployer.GPU_SPECS.keys()),
        help='GPU å‹å·'
    )
    parser.add_argument('--gpu-count', type=int, default=1, help='GPU æ•°é‡ (é»˜è®¤: 1)')
    
    # èµ„æºé…ç½®ï¼ˆæ˜¾å¼æŒ‡å®šæ¨¡å¼ï¼‰
    parser.add_argument('--cpu', help='CPU æ ¸æ•° (å¦‚: 8, 16)')
    parser.add_argument('--memory', help='å†…å­˜å¤§å° (å¦‚: 32Gi, 64Gi)')
    parser.add_argument('--no-auto-match', action='store_true', help='ç¦ç”¨è‡ªåŠ¨åŒ¹é…ï¼Œä½¿ç”¨æ˜¾å¼æŒ‡å®šæ¨¡å¼')
    
    # é•œåƒç¼“å­˜é…ç½®
    parser.add_argument('--use-image-cache', action='store_true', help='å¯ç”¨é•œåƒç¼“å­˜')
    parser.add_argument('--image-cache-id', help='é•œåƒç¼“å­˜ ID (å¦‚: imc-xxxxxxxx)')
    parser.add_argument('--disk-size', type=int, default=200, help='ç£ç›˜å¤§å° GB (é»˜è®¤: 200)')
    
    # å®¹å™¨é…ç½®
    parser.add_argument('--command', help='å®¹å™¨å¯åŠ¨å‘½ä»¤ (JSON æ•°ç»„æ ¼å¼)')
    parser.add_argument('--env', help='ç¯å¢ƒå˜é‡ (JSON å¯¹è±¡æ ¼å¼)')
    
    # æ—¥å¿—å‚æ•°
    parser.add_argument('--tail', type=int, default=100, help='æ˜¾ç¤ºæ—¥å¿—è¡Œæ•° (é»˜è®¤: 100)')
    
    args = parser.parse_args()
    
    try:
        deployer = GPUPodDeployer(kubeconfig_path=args.kubeconfig)
        
        # åˆ é™¤æ“ä½œ
        if args.delete:
            deployer.delete_pod(args.name, args.namespace)
            return
        
        # æŸ¥çœ‹æ—¥å¿—
        if args.logs:
            logs = deployer.get_pod_logs(args.name, args.namespace, args.tail)
            print(logs)
            return
        
        # åˆ›å»ºæ“ä½œ
        if not args.image or not args.gpu_type:
            parser.error("åˆ›å»º Pod éœ€è¦ --image å’Œ --gpu-type å‚æ•°")
        
        # è§£æå‘½ä»¤å’Œç¯å¢ƒå˜é‡
        command = None
        if args.command:
            import json
            command = json.loads(args.command)
        
        env = None
        if args.env:
            import json
            env = json.loads(args.env)
        
        # åˆ›å»º GPU Pod
        result = deployer.create_gpu_pod(
            name=args.name,
            namespace=args.namespace,
            image=args.image,
            gpu_type=args.gpu_type,
            gpu_count=args.gpu_count,
            cpu=args.cpu,
            memory=args.memory,
            use_auto_match=not args.no_auto_match,
            use_image_cache=args.use_image_cache or bool(args.image_cache_id),
            image_cache_id=args.image_cache_id,
            disk_size=args.disk_size,
            command=command,
            env=env
        )
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ GPU Pod éƒ¨ç½²å®Œæˆ!")
        logger.info(f"ğŸ“ Pod åç§°: {result['name']}")
        logger.info(f"ğŸ“¦ å‘½åç©ºé—´: {result['namespace']}")
        logger.info(f"ğŸ® GPU ç±»å‹: {result['gpu_type']}")
        logger.info(f"ğŸ”¢ GPU æ•°é‡: {result['gpu_count']}")
        logger.info(f"ğŸ–¼ï¸  é•œåƒ: {result['image']}")
        logger.info("=" * 60)
        logger.info("\næŸ¥çœ‹ Pod çŠ¶æ€:")
        logger.info(f"  kubectl get pod {result['name']} -n {result['namespace']}")
        logger.info("\næŸ¥çœ‹ Pod è¯¦æƒ…:")
        logger.info(f"  kubectl describe pod {result['name']} -n {result['namespace']}")
        logger.info("\næŸ¥çœ‹ Pod æ—¥å¿—:")
        logger.info(f"  kubectl logs {result['name']} -n {result['namespace']}")
        logger.info("\nè¿›å…¥ Pod:")
        logger.info(f"  kubectl exec -it {result['name']} -n {result['namespace']} -- bash")
        logger.info("\næŸ¥çœ‹ GPU ä¿¡æ¯:")
        logger.info(f"  kubectl exec {result['name']} -n {result['namespace']} -- nvidia-smi")
        
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ æ“ä½œå·²å–æ¶ˆ")
        sys.exit(130)
    except Exception as e:
        logger.error(f"âŒ æ“ä½œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()
