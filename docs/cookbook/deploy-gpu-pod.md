# éƒ¨ç½² GPU Pod

!!! abstract "Cookbook æ¦‚è¿°"
    åœ¨ TKE è¶…çº§èŠ‚ç‚¹ä¸Šéƒ¨ç½² GPU Pod çš„å®Œæ•´ç¤ºä¾‹ã€‚æ”¯æŒå¤šç§ GPU å‹å·(V100/T4/A10/L20/L40)ã€vGPU å’Œé•œåƒç¼“å­˜åŠ é€Ÿã€‚

---

## ğŸ“‹ åŠŸèƒ½ç‰¹æ€§

<div class="grid cards" markdown>

- :material-chip:{ .lg .middle } **å¤šGPU æ”¯æŒ**

    ---

    æ”¯æŒ V100ã€T4ã€A10ã€L20ã€L40 å’Œ vGPU (1/4*T4, 1/2*T4)

- :material-flash:{ .lg .middle } **é•œåƒç¼“å­˜**

    ---

    æ”¯æŒè‡ªåŠ¨åˆ›å»ºå’Œä½¿ç”¨é•œåƒç¼“å­˜ï¼Œç§’çº§å¯åŠ¨ Pod

- :material-tune:{ .lg .middle } **èµ„æºè‡ªåŠ¨åŒ¹é…**

    ---

    æ ¹æ® GPU å‹å·è‡ªåŠ¨åŒ¹é…æœ€ä¼˜ CPU å’Œå†…å­˜é…ç½®

- :material-layers:{ .lg .middle } **ç”Ÿäº§å°±ç»ª**

    ---

    åŒ…å«å¥åº·æ£€æŸ¥ã€èµ„æºé™åˆ¶å’Œå®Œæ•´é”™è¯¯å¤„ç†

</div>

---

## ğŸ—ï¸ æ¶æ„å›¾

```mermaid
graph TB
    A[Python è„šæœ¬<br/>deploy_gpu_pod.py] -->|è°ƒç”¨| B[Kubernetes API]
    
    B -->|åˆ›å»º| C[Pod<br/>gpu-workload]
    C -->|è°ƒåº¦åˆ°| D[è¶…çº§èŠ‚ç‚¹<br/>Virtual Kubelet]
    
    D --> E[GPU èµ„æºåˆ†é…]
    E --> F{GPU å‹å·}
    
    F -->|æ•´å¡| G1[V100 16GB]
    F -->|æ•´å¡| G2[T4 16GB]
    F -->|æ•´å¡| G3[A10 24GB]
    F -->|vGPU| G4[1/4*T4 4GB]
    F -->|vGPU| G5[1/2*T4 8GB]
    
    C --> H[é•œåƒæ‹‰å–ä¼˜åŒ–]
    H -->|å¯ç”¨ç¼“å­˜| I[é•œåƒç¼“å­˜<br/>imc-xxxxxxxx]
    H -->|æ— ç¼“å­˜| J[ç›´æ¥æ‹‰å–<br/>3-5åˆ†é’Ÿ]
    I -->|åŠ é€Ÿ| K[ç§’çº§å¯åŠ¨]
    
    C --> L[èµ„æºé…ç½®]
    L --> L1[è‡ªåŠ¨åŒ¹é…æ¨¡å¼<br/>æ¨è CPU/å†…å­˜]
    L --> L2[æ˜¾å¼æŒ‡å®šæ¨¡å¼<br/>è‡ªå®šä¹‰èµ„æº]
    
    C --> M[å®¹å™¨è¿è¡Œæ—¶]
    M --> M1[CUDA ç¯å¢ƒ]
    M --> M2[NVIDIA Driver]
    M --> M3[GPU è®¾å¤‡æŒ‚è½½]
    
    style A fill:#4051b5,stroke:#333,stroke-width:2px,color:#fff
    style C fill:#ff6f00,stroke:#333,stroke-width:2px,color:#fff
    style D fill:#ff6f00,stroke:#333,stroke-width:2px,color:#fff
    style G1 fill:#4caf50,stroke:#333,stroke-width:2px,color:#fff
    style G2 fill:#4caf50,stroke:#333,stroke-width:2px,color:#fff
    style G3 fill:#4caf50,stroke:#333,stroke-width:2px,color:#fff
    style I fill:#2196f3,stroke:#333,stroke-width:2px,color:#fff
    style K fill:#2196f3,stroke:#333,stroke-width:2px,color:#fff
```

**å·¥ä½œæµç¨‹**:

1. **è„šæœ¬åˆå§‹åŒ–**: åŠ è½½ kubeconfig å’Œå‚æ•°é…ç½®
2. **æ„é€  Pod**: æ ¹æ® GPU å‹å·å’Œèµ„æºè¦æ±‚æ„é€  Pod å®šä¹‰
3. **è°ƒåº¦åˆ°è¶…çº§èŠ‚ç‚¹**: é€šè¿‡ nodeSelector å’Œ tolerations è°ƒåº¦
4. **GPU èµ„æºåˆ†é…**: è…¾è®¯äº‘åå°åˆ†é…æŒ‡å®šçš„ GPU èµ„æº
5. **é•œåƒæ‹‰å–**: ä½¿ç”¨ç¼“å­˜åŠ é€Ÿæˆ–ç›´æ¥æ‹‰å–é•œåƒ
6. **å®¹å™¨å¯åŠ¨**: æŒ‚è½½ GPU è®¾å¤‡å¹¶å¯åŠ¨å®¹å™¨

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®æ¡ä»¶

- **Python 3.8+**
- **kubectl** (å·²é…ç½® kubeconfig)
- **TKE é›†ç¾¤** (å·²å¯ç”¨è¶…çº§èŠ‚ç‚¹)
- **è¶…çº§èŠ‚ç‚¹æ± ** ([åˆ›å»ºè¶…çº§èŠ‚ç‚¹æ± ](../basics/supernode/01-create-supernode-pool.md))

### å®‰è£…ä¾èµ–

```bash
cd tke-workshop.github.io/cookbook

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

---

## ğŸ’» ä½¿ç”¨æ–¹æ³•

### åŸºç¡€ç”¨æ³• (è‡ªåŠ¨åŒ¹é…)

```bash
# åˆ›å»º T4 GPU Pod (è‡ªåŠ¨åŒ¹é… CPU/å†…å­˜)
python3 supernode/deploy_gpu_pod.py \
  --name gpu-inference \
  --image pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime \
  --gpu-type T4 \
  --gpu-count 1
```

### æ˜¾å¼æŒ‡å®šèµ„æº

```bash
# ç²¾ç¡®æ§åˆ¶èµ„æºé…ç½®
python3 supernode/deploy_gpu_pod.py \
  --name gpu-training \
  --image tensorflow/tensorflow:2.13.0-gpu \
  --gpu-type V100 \
  --gpu-count 1 \
  --cpu 8 \
  --memory 40Gi \
  --no-auto-match
```

### ä½¿ç”¨é•œåƒç¼“å­˜

```bash
# è‡ªåŠ¨åˆ›å»ºé•œåƒç¼“å­˜
python3 supernode/deploy_gpu_pod.py \
  --name gpu-fast \
  --image pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime \
  --gpu-type T4 \
  --use-image-cache \
  --disk-size 200
```

### å‚æ•°è¯´æ˜

| å‚æ•° | å¿…å¡« | é»˜è®¤å€¼ | è¯´æ˜ |
| --- | --- | --- | --- |
| `--name` | âœ… | - | Pod åç§° |
| `--image` | âœ… | - | å®¹å™¨é•œåƒ |
| `--gpu-type` | âœ… | - | GPU å‹å· (V100/T4/A10ç­‰) |
| `--gpu-count` | âŒ | `1` | GPU æ•°é‡ |
| `--cpu` | âŒ | è‡ªåŠ¨åŒ¹é… | CPU æ ¸æ•° |
| `--memory` | âŒ | è‡ªåŠ¨åŒ¹é… | å†…å­˜å¤§å° |
| `--no-auto-match` | âŒ | `False` | ç¦ç”¨è‡ªåŠ¨åŒ¹é… |
| `--use-image-cache` | âŒ | `False` | ä½¿ç”¨é•œåƒç¼“å­˜ |
| `--image-cache-id` | âŒ | - | æŒ‡å®šé•œåƒç¼“å­˜ ID |
| `--disk-size` | âŒ | `100` | ç£ç›˜å¤§å°(GB) |
| `--command` | âŒ | - | å®¹å™¨å¯åŠ¨å‘½ä»¤(JSON) |
| `--env` | âŒ | - | ç¯å¢ƒå˜é‡(JSON) |
| `--logs` | âŒ | `False` | æŸ¥çœ‹ Pod æ—¥å¿— |
| `--delete` | âŒ | `False` | åˆ é™¤ Pod |

---

## ğŸ“ å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹ 1: AI æ¨ç†æœåŠ¡

```bash
# éƒ¨ç½²å¤§æ¨¡å‹æ¨ç†æœåŠ¡
python3 supernode/deploy_gpu_pod.py \
  --name llm-inference \
  --image vllm/vllm-openai:latest \
  --gpu-type A10*GNV4 \
  --gpu-count 1 \
  --use-image-cache \
  --disk-size 300 \
  --env '{"MODEL_NAME": "meta-llama/Llama-2-7b-hf", "MAX_MODEL_LEN": "4096"}'

# æŸ¥çœ‹å¯åŠ¨æ—¥å¿—
kubectl logs llm-inference -f

# æµ‹è¯• GPU
kubectl exec llm-inference -- nvidia-smi
```

### ç¤ºä¾‹ 2: æ¨¡å‹è®­ç»ƒ

```bash
# åˆ›å»º 2å¡ V100 è®­ç»ƒ Pod
python3 supernode/deploy_gpu_pod.py \
  --name model-training \
  --image tensorflow/tensorflow:2.13.0-gpu \
  --gpu-type V100 \
  --gpu-count 2 \
  --cpu 18 \
  --memory 80Gi \
  --no-auto-match \
  --use-image-cache \
  --disk-size 500

# ç›‘æ§è®­ç»ƒè¿›åº¦
kubectl logs model-training -f

# æŸ¥çœ‹ GPU ä½¿ç”¨ç‡
kubectl exec model-training -- nvidia-smi
```

### ç¤ºä¾‹ 3: vGPU æ‰¹å¤„ç†

```bash
# ä½¿ç”¨ 1/4 T4 é™ä½æˆæœ¬
for i in {1..5}; do
  python3 supernode/deploy_gpu_pod.py \
    --name gpu-batch-$i \
    --image pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime \
    --gpu-type "1/4*T4" \
    --use-image-cache \
    --disk-size 150 &
done
wait

# æŸ¥çœ‹æ‰€æœ‰æ‰¹å¤„ç† Pod
kubectl get pods -l scenario=batch-inference
```

### ç¤ºä¾‹ 4: è‡ªå®šä¹‰å‘½ä»¤å’Œç¯å¢ƒå˜é‡

```bash
python3 supernode/deploy_gpu_pod.py \
  --name gpu-custom \
  --image pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime \
  --gpu-type T4 \
  --command '["python3", "-m", "http.server", "8080"]' \
  --env '{"MODEL_PATH": "/models/bert", "BATCH_SIZE": "32"}'
```

---

## ğŸ“Š æ”¯æŒçš„ GPU å‹å·

| GPU å‹å· | æ˜¾å­˜ | CUDA | é€‚ç”¨åœºæ™¯ | æ¨èé…ç½® |
| --- | --- | --- | --- | --- |
| **V100** | 16GB | 11.4 | é«˜æ€§èƒ½è®­ç»ƒã€å¤§æ¨¡å‹æ¨ç† | 8æ ¸/40GiB |
| **T4** | 16GB | 11.4 | é€šç”¨æ¨ç†ã€å°æ¨¡å‹è®­ç»ƒ | 8æ ¸/32GiB |
| **1/4*T4** | 4GB | 11.0 | è½»é‡æ¨ç†ã€å¼€å‘æµ‹è¯• | 4æ ¸/16GiB |
| **1/2*T4** | 8GB | 11.0 | ä¸­ç­‰æ¨ç†ã€æ‰¹å¤„ç† | 8æ ¸/32GiB |
| **A10*GNV4** | 24GB | 11.4 | AI æ¨ç†ã€å›¾å½¢æ¸²æŸ“ | 12æ ¸/44GiB |
| **A10*GNV4v** | 24GB | 11.4 | è™šæ‹ŸåŒ– GPU å·¥ä½œè´Ÿè½½ | 28æ ¸/116GiB |
| **L20** | 48GB | 12.7 | é«˜ç«¯å›¾å½¢å·¥ä½œè´Ÿè½½ | 48æ ¸/192GiB |
| **L40** | 48GB | 12.7 | é«˜ç«¯å›¾å½¢å·¥ä½œè´Ÿè½½ | 48æ ¸/192GiB |

å®Œæ•´è§„æ ¼è¡¨è¯·å‚è€ƒ: [GPU Pod æœ€ä½³å®è·µæ–‡æ¡£](../ai-ml/04-gpu-pod-best-practices.md)

---

## âš¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. ä½¿ç”¨é•œåƒç¼“å­˜

```bash
# æ¨èç£ç›˜å¤§å°é…ç½®
- å°é•œåƒ (< 2GB): 50GB
- ä¸­ç­‰é•œåƒ (2-5GB): 100GB
- å¤§é•œåƒ (5-10GB): 200GB
- è¶…å¤§é•œåƒ (> 10GB): 300GB+
```

### 2. é€‰æ‹©åˆé€‚çš„ GPU å‹å·

```bash
# è½»é‡æ¨ç† â†’ ä½¿ç”¨ vGPU
--gpu-type "1/4*T4"  # èŠ‚çœ 75% GPU æˆæœ¬

# é€šç”¨æ¨ç† â†’ ä½¿ç”¨ T4
--gpu-type T4

# å¤§æ¨¡å‹æ¨ç†/è®­ç»ƒ â†’ ä½¿ç”¨ V100/A10
--gpu-type V100
--gpu-type A10*GNV4v
```

### 3. èµ„æºé…ç½®æœ€ä½³å®è·µ

```bash
# å¼€å‘æµ‹è¯•ï¼šä½¿ç”¨è‡ªåŠ¨åŒ¹é…
--gpu-type T4  # ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„ CPU/å†…å­˜

# ç”Ÿäº§ç¯å¢ƒï¼šä½¿ç”¨æ˜¾å¼æŒ‡å®š
--gpu-type V100 --cpu 8 --memory 40Gi --no-auto-match
```

---

## ğŸ“‚ é¡¹ç›®ç»“æ„

```
cookbook/supernode/
â”œâ”€â”€ deploy_gpu_pod.py           # æœ¬è„šæœ¬
â”œâ”€â”€ gpu_pod_examples.yaml       # YAML é…ç½®ç¤ºä¾‹
â””â”€â”€ README.md                   # è¯¦ç»†æ–‡æ¡£

cookbook/common/
â”œâ”€â”€ logger.py                   # æ—¥å¿—å·¥å…·
â””â”€â”€ __init__.py
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: Pod ä¸€ç›´ Pending

```bash
# æŸ¥çœ‹ Pod äº‹ä»¶
kubectl describe pod <pod-name>

# å¸¸è§åŸå› :
# - è¶…çº§èŠ‚ç‚¹æ± èµ„æºä¸è¶³
# - GPU å‹å·é…ç½®é”™è¯¯
# - æœªæ­£ç¡®é…ç½® nodeSelector å’Œ tolerations
```

### é—®é¢˜ 2: GPU ä¸å¯ç”¨

```bash
# æ£€æŸ¥ GPU æ˜¯å¦è¢«è¯†åˆ«
kubectl exec <pod-name> -- nvidia-smi

# æ£€æŸ¥ç¯å¢ƒå˜é‡
kubectl exec <pod-name> -- env | grep NVIDIA

# ç¡®ä¿è®¾ç½®äº†ä»¥ä¸‹ç¯å¢ƒå˜é‡:
# NVIDIA_VISIBLE_DEVICES=all
# NVIDIA_DRIVER_CAPABILITIES=compute,utility
```

### é—®é¢˜ 3: é•œåƒæ‹‰å–å¤±è´¥

```bash
# æŸ¥çœ‹è¯¦ç»†é”™è¯¯
kubectl describe pod <pod-name>

# è§£å†³æ–¹æ¡ˆ:
# 1. æ£€æŸ¥é•œåƒåœ°å€æ˜¯å¦æ­£ç¡®
# 2. ç¡®è®¤ç½‘ç»œè¿æ¥(å¯èƒ½éœ€è¦ EIP)
# 3. ç§æœ‰é•œåƒéœ€è¦é…ç½® imagePullSecrets
```

### é—®é¢˜ 4: é•œåƒç¼“å­˜æœªç”Ÿæ•ˆ

```bash
# æŸ¥çœ‹ Pod äº‹ä»¶,ç¡®è®¤æ˜¯å¦ä½¿ç”¨äº†ç¼“å­˜
kubectl describe pod <pod-name> | grep -i cache

# æ£€æŸ¥ç‚¹:
# 1. é•œåƒåç§°å’Œç‰ˆæœ¬æ˜¯å¦å®Œå…¨åŒ¹é…
# 2. ç£ç›˜å¤§å°æ˜¯å¦ä¸ç¼“å­˜ä¸€è‡´
# 3. é•œåƒç¼“å­˜çŠ¶æ€æ˜¯å¦ä¸º Ready
```

---

## ğŸ§ª éªŒè¯å’Œæµ‹è¯•

### éªŒè¯ GPU å¯ç”¨æ€§

```bash
# 1. æŸ¥çœ‹ Pod çŠ¶æ€
kubectl get pod <pod-name>

# 2. è¿›å…¥ Pod å¹¶æ£€æŸ¥ GPU
kubectl exec -it <pod-name> -- nvidia-smi

# 3. è¿è¡Œ GPU æµ‹è¯•
kubectl exec <pod-name> -- python3 -c "import torch; print(torch.cuda.is_available())"

# 4. æŸ¥çœ‹ Pod äº‹ä»¶
kubectl describe pod <pod-name>
```

### æ€§èƒ½æµ‹è¯•

```bash
# GPU å†…å­˜å¸¦å®½æµ‹è¯•
kubectl exec <pod-name> -- nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv

# CUDA è®¡ç®—æµ‹è¯•
kubectl exec <pod-name> -- python3 -c "
import torch
x = torch.rand(5000, 5000).cuda()
y = torch.rand(5000, 5000).cuda()
z = torch.matmul(x, y)
print('GPU compute test passed!')
"
```

---

## ğŸ”— ç›¸å…³é“¾æ¥

- **æºä»£ç **: [cookbook/supernode/deploy_gpu_pod.py](https://github.com/tke-workshop/tke-workshop.github.io/blob/main/cookbook/supernode/deploy_gpu_pod.py)
- **YAML é…ç½®**: [cookbook/supernode/gpu_pod_examples.yaml](https://github.com/tke-workshop/tke-workshop.github.io/blob/main/cookbook/supernode/gpu_pod_examples.yaml)
- **æ–‡æ¡£**: [GPU Pod æœ€ä½³å®è·µ](../ai-ml/04-gpu-pod-best-practices.md)
- **åˆ›å»ºè¶…çº§èŠ‚ç‚¹æ± **: [åŸºç¡€æ•™ç¨‹](../basics/supernode/01-create-supernode-pool.md)
- **é•œåƒç¼“å­˜æ–‡æ¡£**: [è…¾è®¯äº‘æ–‡æ¡£](https://cloud.tencent.com/document/product/457/65908)
- **è¿”å› Cookbook åˆ—è¡¨**: [Cookbook é›†åˆ](index.md)

---

## ğŸ¤ è´¡çŒ®

å‘ç° Bug æˆ–æœ‰æ”¹è¿›å»ºè®®? æ¬¢è¿æäº¤ Issue æˆ– Pull Request!

[:material-github: æŸ¥çœ‹æºä»£ç ](https://github.com/tke-workshop/tke-workshop.github.io/tree/main/cookbook/supernode){ .md-button .md-button--primary }
[:material-bug: æŠ¥å‘Šé—®é¢˜](https://github.com/tke-workshop/tke-workshop.github.io/issues){ .md-button }

---

â† [éƒ¨ç½² Nginx åº”ç”¨](deploy-nginx.md) | **è¿”å›**: [Cookbook é›†åˆ](index.md)
