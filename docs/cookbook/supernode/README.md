# è¶…çº§èŠ‚ç‚¹ Cookbook

> ğŸš€ **è¶…çº§èŠ‚ç‚¹ GPU Pod éƒ¨ç½²è„šæœ¬å’Œé…ç½®ç¤ºä¾‹**

## ğŸ“– ç®€ä»‹

æœ¬ç›®å½•æä¾›åœ¨ TKE è¶…çº§èŠ‚ç‚¹ä¸Šéƒ¨ç½² GPU Pod çš„å®Œæ•´ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ Python è„šæœ¬å’Œ YAML é…ç½®æ–‡ä»¶ã€‚

**æ–‡æ¡£é“¾æ¥**: [GPU Pod æœ€ä½³å®è·µ](https://tke-workshop.github.io/ai-ml/04-gpu-pod-best-practices/)

---

## ğŸ“‚ æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | ç±»å‹ | åŠŸèƒ½ | æ¨èåœºæ™¯ |
|------|------|------|---------|
| `deploy_gpu_pod.py` | Python | GPU Pod éƒ¨ç½²è„šæœ¬ | è‡ªåŠ¨åŒ–éƒ¨ç½²ã€æ‰¹é‡æ“ä½œ |
| `gpu_pod_examples.yaml` | YAML | GPU Pod é…ç½®ç¤ºä¾‹é›†åˆ | kubectl ç›´æ¥éƒ¨ç½² |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®æ¡ä»¶

1. **Python 3.8+** (ä½¿ç”¨ Python è„šæœ¬)
2. **kubectl** (å·²é…ç½® kubeconfig)
3. **TKE é›†ç¾¤** (å·²å¯ç”¨è¶…çº§èŠ‚ç‚¹)
4. **è¶…çº§èŠ‚ç‚¹æ± ** (å‚è€ƒ [åˆ›å»ºè¶…çº§èŠ‚ç‚¹æ± ](https://tke-workshop.github.io/basics/supernode/01-create-supernode-pool/))

### å®‰è£…ä¾èµ–

```bash
# å®‰è£… Python ä¾èµ–
pip install -r ../requirements.txt
```

---

## ğŸ“ ä½¿ç”¨æ–¹æ³•

### æ–¹å¼ä¸€ï¼šä½¿ç”¨ Python è„šæœ¬ï¼ˆæ¨èï¼‰

#### 1. åŸºç¡€ç”¨æ³•ï¼šè‡ªåŠ¨åŒ¹é…æ¨¡å¼

```bash
# åˆ›å»º T4 GPU Podï¼ˆè‡ªåŠ¨åŒ¹é…èµ„æºï¼‰
python3 deploy_gpu_pod.py \
  --name gpu-inference \
  --image pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime \
  --gpu-type T4 \
  --gpu-count 1
```

#### 2. æ˜¾å¼æŒ‡å®šæ¨¡å¼

```bash
# ç²¾ç¡®æ§åˆ¶ CPU å’Œå†…å­˜é…ç½®
python3 deploy_gpu_pod.py \
  --name gpu-training \
  --image tensorflow/tensorflow:2.13.0-gpu \
  --gpu-type V100 \
  --gpu-count 1 \
  --cpu 8 \
  --memory 40Gi \
  --no-auto-match
```

#### 3. ä½¿ç”¨é•œåƒç¼“å­˜åŠ é€Ÿ

```bash
# è‡ªåŠ¨é•œåƒç¼“å­˜
python3 deploy_gpu_pod.py \
  --name gpu-fast \
  --image pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime \
  --gpu-type T4 \
  --use-image-cache \
  --disk-size 200

# æ‰‹åŠ¨æŒ‡å®šé•œåƒç¼“å­˜ ID
python3 deploy_gpu_pod.py \
  --name gpu-cached \
  --image pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime \
  --gpu-type T4 \
  --image-cache-id imc-xxxxxxxx
```

#### 4. å¤š GPU è®­ç»ƒ

```bash
# åˆ›å»º 2 å¡ V100 è®­ç»ƒ Pod
python3 deploy_gpu_pod.py \
  --name gpu-multi-training \
  --image nvcr.io/nvidia/pytorch:23.08-py3 \
  --gpu-type V100 \
  --gpu-count 2
```

#### 5. vGPU Podï¼ˆæˆæœ¬ä¼˜åŒ–ï¼‰

```bash
# ä½¿ç”¨ 1/4 T4 å¡
python3 deploy_gpu_pod.py \
  --name gpu-vgpu \
  --image pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime \
  --gpu-type "1/4*T4" \
  --gpu-count 1
```

#### 6. è‡ªå®šä¹‰å‘½ä»¤å’Œç¯å¢ƒå˜é‡

```bash
# ä¼ é€’å¯åŠ¨å‘½ä»¤å’Œç¯å¢ƒå˜é‡
python3 deploy_gpu_pod.py \
  --name gpu-custom \
  --image pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime \
  --gpu-type T4 \
  --command '["python3", "-m", "http.server", "8080"]' \
  --env '{"MODEL_PATH": "/models/bert", "BATCH_SIZE": "32"}'
```

#### 7. ç®¡ç†æ“ä½œ

```bash
# æŸ¥çœ‹ Pod æ—¥å¿—
python3 deploy_gpu_pod.py --logs --name gpu-inference --tail 100

# åˆ é™¤ Pod
python3 deploy_gpu_pod.py --delete --name gpu-inference
```

#### 8. æŸ¥çœ‹å¸®åŠ©

```bash
python3 deploy_gpu_pod.py --help
```

### æ–¹å¼äºŒï¼šä½¿ç”¨ YAML é…ç½®

#### 1. éƒ¨ç½²å•ä¸ªç¤ºä¾‹

```bash
# éƒ¨ç½²è‡ªåŠ¨åŒ¹é… GPU Pod
kubectl apply -f gpu_pod_examples.yaml

# æˆ–éƒ¨ç½²ç‰¹å®šç¤ºä¾‹
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: gpu-test
  annotations:
    eks.tke.cloud.tencent.com/gpu-type: T4
spec:
  containers:
  - name: test
    image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime
    command: ["sleep", "3600"]
    resources:
      limits:
        nvidia.com/gpu: 1
  nodeSelector:
    type: virtual-kubelet
  tolerations:
  - key: serverless
    operator: Exists
    effect: NoSchedule
EOF
```

#### 2. éªŒè¯ GPU å¯ç”¨æ€§

```bash
# æŸ¥çœ‹ Pod çŠ¶æ€
kubectl get pod gpu-test

# è¿›å…¥ Pod å¹¶æ£€æŸ¥ GPU
kubectl exec -it gpu-test -- nvidia-smi

# æŸ¥çœ‹ Pod äº‹ä»¶
kubectl describe pod gpu-test
```

---

## ğŸ¯ å®Œæ•´ç¤ºä¾‹åœºæ™¯

### åœºæ™¯ 1: AI æ¨ç†æœåŠ¡

```bash
# 1. åˆ›å»ºæ¨ç† Podï¼ˆä½¿ç”¨é•œåƒç¼“å­˜ï¼‰
python3 deploy_gpu_pod.py \
  --name llm-inference \
  --image vllm/vllm-openai:latest \
  --gpu-type A10*GNV4 \
  --gpu-count 1 \
  --use-image-cache \
  --disk-size 300 \
  --env '{"MODEL_NAME": "meta-llama/Llama-2-7b-hf", "MAX_MODEL_LEN": "4096"}'

# 2. æŸ¥çœ‹å¯åŠ¨æ—¥å¿—
kubectl logs llm-inference -f

# 3. æµ‹è¯• GPU
kubectl exec llm-inference -- nvidia-smi

# 4. è½¬å‘ç«¯å£ï¼ˆå¦‚éœ€æœ¬åœ°è®¿é—®ï¼‰
kubectl port-forward llm-inference 8000:8000

# 5. æ¸…ç†
python3 deploy_gpu_pod.py --delete --name llm-inference
```

### åœºæ™¯ 2: æ¨¡å‹è®­ç»ƒä»»åŠ¡

```bash
# 1. åˆ›å»ºè®­ç»ƒ Pod
python3 deploy_gpu_pod.py \
  --name model-training \
  --image tensorflow/tensorflow:2.13.0-gpu \
  --gpu-type V100 \
  --gpu-count 2 \
  --cpu 18 \
  --memory 80Gi \
  --no-auto-match \
  --use-image-cache \
  --disk-size 500

# 2. ç›‘æ§è®­ç»ƒè¿›åº¦
kubectl logs model-training -f

# 3. æŸ¥çœ‹ GPU ä½¿ç”¨ç‡
kubectl exec model-training -- nvidia-smi

# 4. è®­ç»ƒå®Œæˆåä¸‹è½½æ¨¡å‹ï¼ˆéœ€è¦é…ç½®å­˜å‚¨å·ï¼‰
kubectl cp model-training:/output/model.pth ./model.pth
```

### åœºæ™¯ 3: æ‰¹é‡æ¨ç†ä»»åŠ¡

```bash
# ä½¿ç”¨ vGPU é™ä½æˆæœ¬
for i in {1..5}; do
  python3 deploy_gpu_pod.py \
    --name gpu-batch-$i \
    --image pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime \
    --gpu-type "1/4*T4" \
    --use-image-cache \
    --disk-size 150 &
done

# ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
wait

# æŸ¥çœ‹æ‰€æœ‰æ‰¹å¤„ç† Pod
kubectl get pods -l scenario=batch-inference
```

---

## ğŸ“Š æ”¯æŒçš„ GPU å‹å·

| GPU å‹å· | æ˜¾å­˜ | CUDA | é€‚ç”¨åœºæ™¯ | æ¨èé…ç½® |
|---------|------|------|---------|---------|
| **V100** | 16GB | 11.4 | é«˜æ€§èƒ½è®­ç»ƒã€å¤§æ¨¡å‹æ¨ç† | 8æ ¸/40GiB |
| **T4** | 16GB | 11.4 | é€šç”¨æ¨ç†ã€å°æ¨¡å‹è®­ç»ƒ | 8æ ¸/32GiB |
| **1/4*T4** | 4GB | 11.0 | è½»é‡æ¨ç†ã€å¼€å‘æµ‹è¯• | 4æ ¸/16GiB |
| **1/2*T4** | 8GB | 11.0 | ä¸­ç­‰æ¨ç†ã€æ‰¹å¤„ç† | 8æ ¸/32GiB |
| **A10*GNV4** | 24GB | 11.4 | AI æ¨ç†ã€å›¾å½¢æ¸²æŸ“ | 12æ ¸/44GiB |
| **A10*GNV4v** | 24GB | 11.4 | è™šæ‹ŸåŒ– GPU å·¥ä½œè´Ÿè½½ | 28æ ¸/116GiB |
| **A10*PNV4** | 24GB | 11.4 | é«˜æ€§èƒ½å›¾å½¢å’Œè®¡ç®— | 28æ ¸/116GiB |
| **L20** | 48GB | 12.7 | é«˜ç«¯å›¾å½¢å·¥ä½œè´Ÿè½½ | 48æ ¸/192GiB |
| **L40** | 48GB | 12.7 | é«˜ç«¯å›¾å½¢å·¥ä½œè´Ÿè½½ | 48æ ¸/192GiB |

å®Œæ•´è§„æ ¼è¡¨è¯·å‚è€ƒ: [GPU Pod æœ€ä½³å®è·µæ–‡æ¡£](https://tke-workshop.github.io/ai-ml/04-gpu-pod-best-practices/#æ”¯æŒçš„-gpu-è§„æ ¼)

---

## âš¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. ä½¿ç”¨é•œåƒç¼“å­˜

```bash
# æ¨èç£ç›˜å¤§å°é…ç½®
- å°é•œåƒ (< 2GB): 50GB
- ä¸­ç­‰é•œåƒ (2-5GB): 100GB
- å¤§é•œåƒ (5-10GB): 200GB
- è¶…å¤§é•œåƒ (> 10GB): 300GB+
```

### 2. é€‰æ‹©åˆé€‚çš„ GPU å‹å·

```bash
# è½»é‡æ¨ç† â†’ ä½¿ç”¨ vGPU
--gpu-type "1/4*T4"  # èŠ‚çœ 75% GPU æˆæœ¬

# é€šç”¨æ¨ç† â†’ ä½¿ç”¨ T4
--gpu-type T4

# å¤§æ¨¡å‹æ¨ç†/è®­ç»ƒ â†’ ä½¿ç”¨ V100/A10
--gpu-type V100
--gpu-type A10*GNV4v
```

### 3. èµ„æºé…ç½®æœ€ä½³å®è·µ

```bash
# å¼€å‘æµ‹è¯•ï¼šä½¿ç”¨è‡ªåŠ¨åŒ¹é…
--gpu-type T4  # ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„ CPU/å†…å­˜

# ç”Ÿäº§ç¯å¢ƒï¼šä½¿ç”¨æ˜¾å¼æŒ‡å®š
--gpu-type V100 --cpu 8 --memory 40Gi --no-auto-match
```

---

## ğŸ” æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: Pod ä¸€ç›´å¤„äº Pending çŠ¶æ€

```bash
# æŸ¥çœ‹ Pod äº‹ä»¶
kubectl describe pod <pod-name>

# å¸¸è§åŸå› ï¼š
# - è¶…çº§èŠ‚ç‚¹æ± èµ„æºä¸è¶³
# - GPU å‹å·é…ç½®é”™è¯¯
# - æœªæ­£ç¡®é…ç½® nodeSelector å’Œ tolerations
```

### é—®é¢˜ 2: GPU ä¸å¯ç”¨

```bash
# æ£€æŸ¥ GPU æ˜¯å¦è¢«è¯†åˆ«
kubectl exec <pod-name> -- nvidia-smi

# æ£€æŸ¥ç¯å¢ƒå˜é‡
kubectl exec <pod-name> -- env | grep NVIDIA

# ç¡®ä¿è®¾ç½®äº†ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š
# NVIDIA_VISIBLE_DEVICES=all
# NVIDIA_DRIVER_CAPABILITIES=compute,utility
```

### é—®é¢˜ 3: é•œåƒæ‹‰å–å¤±è´¥

```bash
# æŸ¥çœ‹è¯¦ç»†é”™è¯¯
kubectl describe pod <pod-name>

# è§£å†³æ–¹æ¡ˆï¼š
# 1. æ£€æŸ¥é•œåƒåœ°å€æ˜¯å¦æ­£ç¡®
# 2. ç¡®è®¤ç½‘ç»œè¿æ¥ï¼ˆå¯èƒ½éœ€è¦ EIPï¼‰
# 3. ç§æœ‰é•œåƒéœ€è¦é…ç½® imagePullSecrets
```

### é—®é¢˜ 4: é•œåƒç¼“å­˜æœªç”Ÿæ•ˆ

```bash
# æŸ¥çœ‹ Pod äº‹ä»¶ï¼Œç¡®è®¤æ˜¯å¦ä½¿ç”¨äº†ç¼“å­˜
kubectl describe pod <pod-name> | grep -i cache

# æ£€æŸ¥ç‚¹ï¼š
# 1. é•œåƒåç§°å’Œç‰ˆæœ¬æ˜¯å¦å®Œå…¨åŒ¹é…
# 2. ç£ç›˜å¤§å°æ˜¯å¦ä¸ç¼“å­˜ä¸€è‡´
# 3. é•œåƒç¼“å­˜çŠ¶æ€æ˜¯å¦ä¸º Ready
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [GPU Pod æœ€ä½³å®è·µ](https://tke-workshop.github.io/ai-ml/04-gpu-pod-best-practices/)
- [åˆ›å»ºè¶…çº§èŠ‚ç‚¹æ± ](https://tke-workshop.github.io/basics/supernode/01-create-supernode-pool/)
- [åˆ›å»ºæŒ‰é‡è¶…çº§èŠ‚ç‚¹](https://tke-workshop.github.io/basics/supernode/02-create-supernode/)
- [é•œåƒç¼“å­˜æ–‡æ¡£](https://cloud.tencent.com/document/product/457/65908)

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

---

## ğŸ“„ è®¸å¯è¯

[Apache License 2.0](../../LICENSE)

---

**ç»´æŠ¤è€…**: TKE Workshop Team  
**æœ€åæ›´æ–°**: 2026-01-08
